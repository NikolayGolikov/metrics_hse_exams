\subsection{ИП, вспомнить всё!}

\begin{enumerate}

  \item Сфорулируйте теорему о трёх перпендикулярах и обратную к ней. Нарисуйте картинку.

  \item Для матрицы
$
  A=\begin{pmatrix}
  4 & 5  \\
  5 & 4  \\
  \end{pmatrix}
$

  \begin{enumerate}
  \item Найдите собственные числа и собственные векторы матрицы;
  \item Найдите определитель $\det A$ и след $\tr A$;
 \item Известно, что $B = A^{-1} + 2018I$, где $I$ — единичная матрица.
 Найдите собственные числа $B$, определитель $\det B$ и след $\tr B$.

  \end{enumerate}


  \item Блондинка Маша встретила 100 динозавров.
  Средний рост динозавров оказался равен 20 метров, а выборочное стандартное отклонение — 5 метров.

  \begin{enumerate}
    \item Постройте 95\% доверительный интервал для математического ожидания роста динозавра.
    \item На уровне значимости 1\% проверьте гипотезу о том, что математическое ожидание
    роста равно 22 метрам. Против альтернативной гипотезе о неравенстве.
    \item Укажите $P$-значение для теста в предыдущем пункте.
  \end{enumerate}

 \item На брег выходят один за одним 33 богатыря. Двадцать вторым по счёту выходит
 богатырь Мефодий. Какова вероятность того, что Мефодий окажется вторым по силе из всех богатырей,
   если известно, что он самый сильный из всех вышедших до него?

\end{enumerate}


\subsection{Вспомнить всё, ответы}

\begin{enumerate}
\item[2.]
\begin{enumerate}
  \item $\lambda^A_1 = -1$, $\lambda^A_2 = 9$,
  $h_1 = \begin{pmatrix}
  1 & -1
  \end{pmatrix}^T$,
  $h_2 = \begin{pmatrix}
  1 & 1
  \end{pmatrix}^T$
  \item $\det(A) = \lambda^A_1 \cdot \lambda^A_2 = -9$, $\tr(A) = \lambda^A_1 +
  \lambda^A_2 = 8$
  \item $\lambda^B_1 = 1 / \lambda^A_1 + 2018 = 2017$,
  $\lambda^B_2 = 1 / \lambda^A_2 + 2018 = 2018 + 1/9$,
  $\det(B) = \lambda^B_1 \cdot \lambda^B_2 = 2017 \cdot (2018 + 1/9)$,
  $\tr(B) = \lambda^B_1 + \lambda^B_2 = 4035 + 1/9$
\end{enumerate}
\item[3.]
\begin{enumerate}
\item $\left[20 - 1.96 \cdot 5 / \sqrt{100}; 20 + 1.96 \cdot 5 / \sqrt{100} \right]$
\item $z_{obs} = -4$, $z_{crit} = \pm 2.6$, основная гипотеза отвергается
\item $p-value \approx 0 $
\end{enumerate}
\item[4.] Заметим, что неважно, каким идёт Мефодий, а главное, что он самый сильный из
$22$ вышедших. Из $22$ вышедших всё равно есть кто-то самый сильный, и если его
считать Мефодием, то ничего не изменится. Значит, нам нужна вероятность того,
что второй лучший из всех попадёт на $22$ места из $33$, а самый лучший на $11$
мест из $32$ оставшихся. Искомая вероятность — $11/48$.

Или по формуле условной вероятности. Пусть $A$ означает, что Мефодий второй по силе
из всех, а $B$ — что он первый по силе из вышедших. Тогда
\[
\P(B) = 1/22,
\]
так как Мефодий должен быть самым сильным из вышедших, и
\[
\P(A \cap B) = 11/33 \cdot 1/32,
\]
так как на $22$-ом месте должен быть самый сильный из вышедших и второй по силе из
всех. Если он второй по силе из вышедших, то первый оказался среди $11$ невышедших,
и эта вероятность равна $11/33$, а вероятность быть вторым по силе среди всех равна
$1/32$. Итого,
\[
\P(A|B) = \frac{\P(A \cap B)}{\P(B)} = \frac{11}{48}.
\]
\end{enumerate}



\subsection{Задачи миниконтрольных ИП}

\begin{enumerate}
  \item Найдите SVD-разложение матрицы $
  \begin{pmatrix}
  2 & 0 & -1 \\
  2 & 1 & 0 \\
  \end{pmatrix}$
 \item Найдите дифференциал $d \cos(r^TAr+br)$, где $A^T=A$ и $b$ — это константы.
 \item Постройте регрессию вектора $y = (4,2,-2)^T$ на вектора $x=(1,0,-1)^T$ и $z=(1,1,-1)^T$
 без константы.
 \item Известно, что $y=x + 2z$. Винни-Пух построил регрессию $\hat y_i = \hat\beta_1 + 0.16 x_i$.
 Пятачок построил регрессию $\hat x_i = \hat \alpha_1 + 1\cdot y_i$.

 Помогите Сове найти коэффициент $\hat \gamma_2$ в регрессии $\hat y_i = \hat\gamma_1 + \hat\gamma_2 z_i$.
\end{enumerate}


\subsection{Контрольная работа-1. Базовая часть}

%\input{tests/2018_2019/tex/exercise01}
%\input{tests/2018_2019/tex/exercise02}
%\input{tests/2018_2019/tex/exercise03}
%\input{tests/2018_2019/tex/exercise04}
%\input{tests/2018_2019/tex/exercise05}
%\input{tests/2018_2019/tex/exercise06}
%\input{tests/2018_2019/tex/exercise07}
%\input{tests/2018_2019/tex/exercise08}
%\input{tests/2018_2019/tex/exercise09}
%\input{tests/2018_2019/tex/exercise10}


\begin{enumerate}
  \item (5 баллов) Случайные величины $X$ и $Y$ независимы и имеют хи-квадрат распределение
  с 5 и с 10 степенями свободы, соответственно. Случайная величина $Z$ равна $Z = (X+Y)/X$.

  Найдите значение $z^*$ такое, что $\P(Z > z^*)=0.05$.
  \item (5 баллов) Докажите, что для модели парной регрессии $Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$,
оцененной с помощью МНК, выполнено равенство $\sum_{i=1}^n Y_i = \sum_{i=1}^n \hat Y_i$.

  \item (5 баллов) Аккуратно сформулируйте теорему Гаусса-Маркова для случая парной регрессии.

  \item (10 баллов) На основании 62 наблюдений Чебурашка оценил функцию спроса на апельсины:

 \[
 \hat Y_i = \underset{(1.6)}{3} - \underset{(0.2)}{1.25} X_i, \text{ где } \sum_i (X_i - \bar X)^2 =2.25
 \]

 В скобках приведены стандартные ошибки коэффициентов, случайные ошибки в регрессии можно считать нормальными.


  \begin{enumerate}
    \item Проверьте гипотезы о значимости каждого из коэффициентов регрессии при уровне значимости 5\%.
    \item Проверьте гипотезу о равенстве коэффициента наклона -1 при уровне значимости 5\%
    и односторонней альтернативной гипотезе, что коэффициент наклона меньше -1.
    \item Найдите оценку дисперсии ошибок.
    \item Найдите 95\% интервальный индивидуальный прогноз в точке $X=8$.
  \end{enumerate}
\end{enumerate}



\subsection{Контрольная работа-1. ИП часть}

\begin{enumerate}
  \item Храбрый исследователь Вениамин поделил выборку на обучающую $(X, y)$ и тестовую $(X_{test}, y_{test})$.
  Регрессоры $X$ и $X_{test}$ Вениамин считает нестохастическими, а предпосылки
  теоремы Гаусса-Маркова — выполненными на всей исходной выборке. Естественно,
  $\hat y_{test} = X_{test}\hat\beta$, где $\hat\beta$ оценивается по обучающей выборке.

  Помогите Вениамину найти $\Var(\hat y_{test})$ и $\Cov(\hat \beta, \hat y_{test})$.

  \item Рассмотрим матрицу $X$ полного ранга с $n$ наблюдениями и $k$ столбцами.
  В каких границах могут лежать диагональные элементы матрицы-шляпницы $H$?
  Чему равно их среднее значение?

  Подсказка: найдите $\Var(\hat y)$ и $\Var(\hat u)$ в рамках предпосылок теоремы Гаусса-Маркова.

  \item Рассмотрим стандартный $t$-тест на равенство некоторого коэффициента бета нулю.
  Докажите, что
  \[
         t^2 = \frac{RSS_r - RSS_{ur}}{RSS_{ur}/(n-k)},
  \]
  где $RSS_r$ — сумма квадратов остатков в модели без тестируемого коэффициента
  (выкинут регрессор при проверямом коэффициенте),
  $RSS_{ur}$ — аналогичная сумма в модели с включённым тестируемым коэффициентом, $k$ —
  число оцениваемых коэффициентов бета в модели с тестируемым коэффициентом, $n$ —
  количество наблюдений.

  Утешительный приз: упростите эту формулу для случая парной регрессии и докажите её :)

  \item Рассмотрим стандартную ошибку оценки коэффициента бета при регрессоре $z$
  в множественной регрессии.
  Докажите, что

  \[
         se^2(\hat\beta_z) = \frac{RSS / (n-k)}{\sum (z_i - \bar z)^2} \cdot \frac{1}{1 - R^2_z},
  \]
  где $R^2_z$ — коэффициент детерминации во вспомогательной регресии объясняющей переменной
  $z$ на остальные объясняющие переменные.

  Утешительный приз: упростите эту формулу для случая парной регрессии и докажите её :)


   \item У Винни-Пуха есть случайный вектор $w$ и одномерная случайную величину $z$.
   Винни-Пуху известны величины $\Cov(w, w) = A$ и $\Cov(w, z) = b$.

   К сожалению, у Винни-Пуха опилки в голове, а он очень хочет найти такую линейную комбинацию
   компонент вектора $w$, которая была бы сильнее всего коррелирована со случайной
   величиной $z$.

   Помогите Винни-Пуху!

   Как выглядят веса этой линейной комбинации?
   Чему равна максимально возможная корреляция?

 \item Машенька построила парную регрессию по 11 наблюдениям с $R^2=
0.95$. Чтобы напакостить Машеньке, Вовочка переставил в случайном
порядке значения зависимой переменной и предложил Машеньке заново оценить модель.

Какой ожидаемый $R^2$ получит Машенька?


<<<<<<< HEAD
\end{enumerate}


\subsection{ИП-часть, решения}
\begin{enumerate}
\item Дисперсия:

\begin{align*}
\Var \left(\hat{y}_{test}\right) &= \Var \left(X_{test} \hat\beta \right) \\
&= X_{test} \Var\left((X'X)^{-1} X' y \right) X'_{test} \\
&= X_{test} (X'X)^{-1} X' \sigma^2 \cdot I X (X'X)^{-1}  X'_{test} \\
&= \sigma^2 X_{test} (X'X)^{-1} X'_{test}
\end{align*}

Ковариация:
\begin{align*}
\Cov\left(\hat \beta, \hat{y}_{test}\right) &= \Cov\left(\hat \beta,  X_{test} \hat{\beta} \right) \\
&= \Var\left(\hat \beta\right) X'_{test} \\
&= \Var\left((X'X)^{-1} X' y \right)X'_{test} \\
&= (X'X)^{-1} X' \sigma^2 \cdot I X (X'X)^{-1}  X'_{test} \\
&= \sigma^2 (X'X)^{-1} X'_{test}
\end{align*}

\item
Воспользуемся подсказкой и найдём:
\begin{align*}
\Var\left(\hat y\right) &= \Var \left(Hy \right) = H \Var(y) H' = \sigma^2 H \\
\Var \left(\hat u \right) &= \Var \left(y - \hat y\right) = \Var \left((I-H)y\right) = \sigma^2 (I-H)
\end{align*}


Соответственно, для диагональных элементов выполнятются соотношения:
\[
\Var\left(\hat y_i \right) = \sigma^2 h_{ii} \text{ и } \Var \left(\hat u_i \right) = \sigma^2(1 - h_{ii}),
\]
откуда следует, что диагональные элементы матрицы-шляпницы лежат в пределах от $0$ до $1$.

Вспомним, что сумму диагональных элементов, или след матрицы, можно вычислить
как сумму её собственных значений.
Поскольку матрица-шляпница является проектором, её собственные числа равны либо
 $0$, либо $1$.
Это легко показать. Пусть $\lambda$ — собственное значение матрицы $H$ с собственным
вектором $v$. Для $H$ выполняется соотношение $H^2 = H$, а значит верно и
\[
\lambda^2 v = H^2 v = H v = \lambda v,
\]
где $v \neq 0$. Отсюда получаем уравнение $\lambda^2 = \lambda$, корни которого —
$0$ и $1$.

Осталось заметить, что для собственных чисел $\lambda = 1$ собственные вектора
лежат в $Lin(X)$, а для нулевых собственных чисел — перпендикулярны ей.
Значит, количество единиц совпадает с размерностью пространства, на которое проецируем.
В нашем случае оно равно $k$. Значит, среднее диагональных элементов — $k/n$.

\item Утверждение можно доказать геометрически.

\begin{align*}
\frac{t}{\sqrt{n-2}} &=
\frac{\hat \beta_2}{\sqrt{n-2}se\left(\hat\beta_2\right)} =
\frac{\hat \beta_2}{\sqrt{n-2}\frac{\hat \sigma}{\sqrt{\sum\limits_{i=1}^n (x_i - \bar x)^2}}}
= \frac{\hat \beta_2 \sqrt{\sum\limits_{i=1}^n (x_i - \bar x)^2}}{\sqrt{n-2}\frac{\sqrt{\sum\limits_{i=1}^n (y_i - \hat y_i)^2}}{\sqrt{n-2}}} \\
&= \frac{\hat \beta_2 \lVert x^c \rVert}{\sqrt{RSS}} = \ctg \varphi\\
\end{align*}

\begin{figure}[ht!]
\begin{center}
\subfigure[]{
\includegraphics[width=0.35\linewidth]{figures/04_ttest.pdf}
\label{fig:ttest_3d}}
%\hspace{4ex}
\subfigure[]{
\includegraphics[width=0.35\linewidth]{figures/04_ttest_lin.pdf}
\label{fig:ttest_lin}}
\caption{\subref{fig:ttest_3d}: Регрессия $y$ на $\Lin(x, \mathbf{1})$ и соответствующие проекции;
\subref{fig:ttest_lin}: $\Lin^{\perp}(\mathbf{1})$.}
\end{center}
\end{figure}

\[
F = \frac{(RSS_{R} - RSS_{UR})/q}{RSS_{UR}/(n-k_{UR})} =
\ctg^2 \varphi \cdot \frac{n - k_{UR}}{q}
\]

\begin{figure}[ht!]
\centering
\includegraphics[scale=0.55]{figures/04_ftest.pdf}
\caption{F-статистика пропорциональна квадрату котангенса $\varphi$,
где $a$ означает $\sqrt{RSS_{UR}}$, $b$ — $\sqrt{RSS_{R} -RSS_{UR}}$,
$c$ — $\sqrt{RSS_{R}}$.}
\label{fig:ftest}
\end{figure}

В случае парной регрессии $k_{UR} = 2, q = 1$.

\item Представим матрицу регрессоров в виде двух блоков: $X = [Z \quad W]$, где $Z$ —
это регрессор (вектор), для которого мы будем искать стандартную ошибку, а
$W$ — это матрица всех остальных регрессоров.
Известно, что
\[
\Var\left(\hat{\beta}_{Z} \right) = \sigma^2 (X'X)^{-1}_{11},
\]
то есть нам нужно найти только первый элемент матрицы $X'X$.
Эта матрица является блочной и в наших обозначениях имеет вид:
\[
X'X = \begin{pmatrix}
Z'Z & Z'W \\
W'Z & W'W
\end{pmatrix}
\]

Чтобы найти обратную матрицу в общем случае, нужно решить систему:
\[
\begin{pmatrix}
A & B \\
C & D
\end{pmatrix}
\cdot
\begin{pmatrix}
E & F \\
G & H
\end{pmatrix}
=
\begin{pmatrix}
I & 0 \\
0 & I
\end{pmatrix},
\]
где матрица слева изветсна, матрица из блоков $E, F, G, H$ является обратной к ней.
Решая эту систему, например, методом Гаусса, получим:
\[
E = (A-BD^{-1}C)^{-1},
\]
а остальные элементы нас не интересуют.
Возвращаясь к нашим обозначениям, запишем:
\[
(X'X)^{-1}_{11} = \left(Z'Z - Z'W \left(W'W \right)^{-1} W'Z \right)
\]
Обозначив $\left(W'W \right)^{-1} W'= H$ (проектор во вспомогательной регрессии на все
переменные, кроме Z), упростим полученное выражение:
\[
(X'X)^{-1}_{11} = Z'(I-H)Z
\]
Как промежуточный итог мы получили:
\[
\Var\left(\hat{\beta}_{Z} \right) = \frac{\sigma^2}{Z'(I-H)Z}
\]
Теперь рассмотрим коэффициент детерминации во вспомогательной регрессии:
\[
1 - R^2_Z = \frac{RSS_Z}{TSS_Z} = \frac{(Z - \hat Z)'(Z - \hat Z)}{(Z - \bar Z)'(Z - \bar Z)} = \frac{((I-H)Z)'(I-H)Z}{(Z - \bar Z)'(Z - \bar Z)}
= \frac{Z'(I-H)Z}{(Z - \bar Z)'(Z - \bar Z)}
\]
Выражая отсюда числетель, получаем:
\[
Z'(I-H)Z = \left( 1 - R^2_Z \right) \cdot \sum (Z_i - \bar Z)^2
\]
Осталось вспомнить оценку для $\sigma^2$ и записать финальный резульат:
\[
\widehat{\Var}\left(\hat{\beta}_{Z} \right) = \frac{RSS/(n-k)}{\sum (Z_i - \bar Z)^2} \cdot \frac{1}{1 - R^2_Z}
\]

\item В том виде, в котором задача дана, она не решается.
Однако можно было выписать целевую функцию и взять её дифференциал:
\[
\Corr(\alpha^T w, z) = \frac{\Cov(\alpha^T w, z)}{\sqrt{\Var(\alpha^T w)\Var(z)}}
= \frac{\alpha^T \Cov(w,z)}{\sqrt{\alpha^T\Var(w) \alpha \Var(z)}} \to \max_{\alpha}
\]
Также можно было заметить, что эта задача напоминает МНК.
Пусть $\hat z$ — проекция $z$ на $\Lin(w_1, \ldots, w_k)$,
а $\tilde{z}$ — произвольный вектор из $\Lin(w_1, \ldots, w_k)$.
Обозначив угол между $z$ и $\hat z$ за $\alpha$, между $z$ и $\tilde z$ за $\beta$,
можем записать:
\[
\Corr(z, \hat z) = \cos \alpha \geq \cos \beta = \Corr(z, \tilde z)
\]
при $\alpha \leq \beta \leq 90^{\circ}$.
И задача максимизации $\Corr(z, \tilde z)$ будет эквивалентна задаче МНК.

\item Без ограничения общности центрируем исходные переменные.
Значения регрессора обозначим $x_i$, исходную зависимую переменную $\tilde{y}_i$,
а переставленную в случайном порядке $y_i$.
По условию, $\sum y_i^2 = \sum \tilde{y}_i^2$.

Фиксируем исходные переменные и находим:
\[
\E(R^2 | \tilde{y}) = \E\left( \frac{(\sum x_i y_i )^2}{\sum x_i^2 \sum y_i^2}  \right) = \frac{\Var(\sum x_i y_i | \tilde{y})}{\sum x_i^2 \sum y_i^2}
\]

Для начала найдём дисперсию отдельного игрека:
\[
\Var(y_i | \tilde{y}) = \frac{\sum \tilde{y}_i^2}{n} = \frac{\sum y_i^2}{n}
\]

А теперь из $\Cov(y_1, \sum y_i | \tilde{y}) =0$ найдём и дисперсию нужной суммы:
\[
\Var(\sum x_i y_i | \tilde{y}) = \Var(y_i | \tilde{y}) \left(  \sum x_i^2 - \sum_{i\neq j} x_i x_j \frac{1}{n-1}  \right) = \Var(y_i | \tilde{y}) \frac{n}{n-1} \sum x_i^2 = \frac{\sum x_i^2 \sum y_i^2 }{n-1}
\]

Заканчиваем подсчёт,
\[
\E(R^2 | \tilde{y}) = \frac{1}{n-1}
\]

Следовательно, и $\E(R^2) = \frac{1}{n-1} = 0.1$.

=======
>>>>>>> 5b941986cd203ebf5f12c669cd3534997fac2f1a
\end{enumerate}
